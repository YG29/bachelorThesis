{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T03:35:45.059655Z",
     "start_time": "2024-06-13T03:35:42.076942Z"
    }
   },
   "id": "7ac30ca417417f7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "xMin = 3\n",
    "xMax = 15\n",
    "nSample = 5000\n",
    "\n",
    "# input\n",
    "x = np.linspace(xMin, xMax, nSample)\n",
    "x_actual = np.linspace(xMin, xMax, nSample)\n",
    "y_actual = f(x)\n",
    "\n",
    "np.random.shuffle(x)\n",
    "\n",
    "np.random.seed(17)\n",
    "epsilon1 = np.random.normal(0.0, 0.3, nSample)\n",
    "epsilon2 = np.random.normal(0.0, 0.3, nSample)\n",
    "\n",
    "y = f(x) + epsilon1 + epsilon2 * x\n",
    "\n",
    "# plt.scatter(x, y, label = \"Dataset\", color = \"pink\", s = 9)\n",
    "# plt.plot(x_actual, y_actual, label = \"Underlying\", color = \"magenta\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "X_train = x.reshape(-1, 1)\n",
    "y_train = y.reshape(-1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_test = np.arange(xMin, xMax+5, 0.01).reshape(-1, 1)\n",
    "y_test = f(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:15:03.704543Z",
     "start_time": "2024-05-16T08:15:03.698087Z"
    }
   },
   "id": "a60fda4d449220e2"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:17:41.263003Z",
     "start_time": "2024-05-16T08:17:41.252248Z"
    }
   },
   "id": "6d6b9a066f547a4c"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 32\n",
    "\n",
    "num_epochs = 20\n",
    "attention_mask = tf.where(tf.math.is_nan(X_train), 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:57:13.111767Z",
     "start_time": "2024-05-16T07:57:13.102208Z"
    }
   },
   "id": "38ac3dc46e07490f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def embed_data(data, tokenizer, max_length):\n",
    "    # Tokenize the data using the specified tokenizer\n",
    "    tokenized_data = tokenizer(\n",
    "        data,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # Convert tokenized inputs to int64 tensors\n",
    "    input_ids = tf.cast(tokenized_data['input_ids'], tf.int64)\n",
    "\n",
    "    return input_ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0f752691573839a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4463800f2d37430e8aa6950baec66dba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe7400e1cb87413baa3f3b822c9f297e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f42042ed6bd54623bebf3a6766e17089"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:05:26.633391Z",
     "start_time": "2024-05-16T08:05:25.518294Z"
    }
   },
   "id": "4d40ed849038a020"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "input_ids = tokenizer(str(X_train), padding=True, truncation=True, return_tensors=\"tf\")[\"input_ids\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:09:02.717803Z",
     "start_time": "2024-05-16T08:09:02.708590Z"
    }
   },
   "id": "8d9f767548cc5a89"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 58)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:16:03.064108Z",
     "start_time": "2024-05-16T08:16:03.054467Z"
    }
   },
   "id": "bf1b23f26e1374f3"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 1 and 4000 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_set \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_tensor_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mbatch(batch_size)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001B[0m, in \u001B[0;36mDatasetV2.from_tensor_slices\u001B[0;34m(tensors, name)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001B[39;00m\n\u001B[1;32m    823\u001B[0m \u001B[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001B[39;00m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001B[39;00m\n\u001B[1;32m    825\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_tensor_slices_op\n\u001B[0;32m--> 826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrom_tensor_slices_op\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_tensor_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001B[0m, in \u001B[0;36m_from_tensor_slices\u001B[0;34m(tensors, name)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_from_tensor_slices\u001B[39m(tensors, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 25\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_TensorSliceDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:45\u001B[0m, in \u001B[0;36m_TensorSliceDataset.__init__\u001B[0;34m(self, element, is_files, name)\u001B[0m\n\u001B[1;32m     42\u001B[0m batch_dim \u001B[38;5;241m=\u001B[39m tensor_shape\u001B[38;5;241m.\u001B[39mDimension(\n\u001B[1;32m     43\u001B[0m     tensor_shape\u001B[38;5;241m.\u001B[39mdimension_value(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_shape()[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensors[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[0;32m---> 45\u001B[0m   \u001B[43mbatch_dim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massert_is_compatible_with\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m      \u001B[49m\u001B[43mtensor_shape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDimension\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m          \u001B[49m\u001B[43mtensor_shape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdimension_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_shape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m variant_tensor \u001B[38;5;241m=\u001B[39m gen_dataset_ops\u001B[38;5;241m.\u001B[39mtensor_slice_dataset(\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensors,\n\u001B[1;32m     51\u001B[0m     output_shapes\u001B[38;5;241m=\u001B[39mstructure\u001B[38;5;241m.\u001B[39mget_flat_tensor_shapes(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_structure),\n\u001B[1;32m     52\u001B[0m     is_files\u001B[38;5;241m=\u001B[39mis_files,\n\u001B[1;32m     53\u001B[0m     metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\u001B[38;5;241m.\u001B[39mSerializeToString())\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(variant_tensor)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:303\u001B[0m, in \u001B[0;36mDimension.assert_is_compatible_with\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001B[39;00m\n\u001B[1;32m    294\u001B[0m \n\u001B[1;32m    295\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;124;03m    is_compatible_with).\u001B[39;00m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_compatible_with(other):\n\u001B[0;32m--> 303\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimensions \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m are not compatible\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m    304\u001B[0m                    (\u001B[38;5;28mself\u001B[39m, other))\n",
      "\u001B[0;31mValueError\u001B[0m: Dimensions 1 and 4000 are not compatible"
     ]
    }
   ],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids}, y_train)).batch(batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:15:13.224260Z",
     "start_time": "2024-05-16T08:15:13.193271Z"
    }
   },
   "id": "b32eb7193f757040"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "# val_set = tf.data.Dataset.from_tensor_slices((X_val,y_val))\n",
    "# test_set = tf.data.Dataset.from_tensor_slices((X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:39:55.306739Z",
     "start_time": "2024-05-16T07:39:55.300Z"
    }
   },
   "id": "53d7b7e4b1a4b894"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_BatchDataset' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m(), padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_BatchDataset' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T08:07:00.075092Z",
     "start_time": "2024-05-16T08:07:00.061099Z"
    }
   },
   "id": "55d0184d88df33e6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_set = train_set.batch(batch_size)\n",
    "val_set = val_set.batch(batch_size)\n",
    "test_set = test_set.batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:39:59.491838Z",
     "start_time": "2024-05-16T07:39:59.487607Z"
    }
   },
   "id": "aecd02dff7786595"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_attention_mask(X):\n",
    "    # Assuming X is a 2D tensor of shape (batch_size, num_features)\n",
    "    attention_mask = tf.cast(tf.not_equal(X, 0), tf.float32)\n",
    "    return attention_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:40:09.269071Z",
     "start_time": "2024-05-16T07:40:09.264793Z"
    }
   },
   "id": "391154c423399595"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset = train_set.map(lambda X, y: (X, create_attention_mask(X), y))\n",
    "val_dataset = val_set.map(lambda X, y: (X, create_attention_mask(X), y))\n",
    "test_dataset = test_set.map(lambda X, y: (X, create_attention_mask(X), y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:40:11.438778Z",
     "start_time": "2024-05-16T07:40:11.384109Z"
    }
   },
   "id": "96bb63cfff1bc25"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    mean, log_var = y_pred\n",
    "    loss = 0.5 * tf.reduce_sum(tf.exp(log_var) + tf.square(y_true - mean) - 1 - log_var, axis=-1)\n",
    "    return tf.reduce_mean(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:40:17.484091Z",
     "start_time": "2024-05-16T07:40:17.478990Z"
    }
   },
   "id": "68b8b74b05a0e80d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CustomMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_metric', **kwargs):\n",
    "        super(CustomMetric, self).__init__(name=name, **kwargs)\n",
    "        self.mse = tf.keras.metrics.MeanSquaredError()\n",
    "        self.coverage = self.add_weight(name='coverage', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mean, log_var = y_pred\n",
    "        std = tf.exp(0.5 * log_var)\n",
    "        coverage = tf.reduce_mean(tf.cast(tf.abs(y_true - mean) <= 1.96 * std, tf.float32))\n",
    "        self.coverage.assign_add(coverage)\n",
    "        self.mse.update_state(y_true, mean, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mse.result(), self.coverage\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mse.reset_state()\n",
    "        self.coverage.assign(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:40:26.540843Z",
     "start_time": "2024-05-16T07:40:26.533572Z"
    }
   },
   "id": "72eee78fe997bcbc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:40:55.458266Z",
     "start_time": "2024-05-16T07:40:54.641144Z"
    }
   },
   "id": "cf669a83e5d52d0a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# try without freezing. "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:41:07.214036Z",
     "start_time": "2024-05-16T07:41:07.207237Z"
    }
   },
   "id": "8a44e877f0026ff6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class BayesianBertRegressor(tf.keras.Model):\n",
    "    def __init__(self, bert_model, dropout_rate=0.1):\n",
    "        super(BayesianBertRegressor, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.mean_layer = Dense(1)\n",
    "        self.log_var_layer = Dense(1)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        output = self.bert(input_ids=input_ids)[0][:, 0]\n",
    "        output = self.dropout(output, training=True)\n",
    "        mean = self.mean_layer(output)\n",
    "        log_var = self.log_var_layer(output)\n",
    "        return mean, log_var\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:47:47.013080Z",
     "start_time": "2024-05-16T07:47:47.006323Z"
    }
   },
   "id": "6950001a561c28e2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "regressor = BayesianBertRegressor(model)\n",
    "regressor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss=loss_function,\n",
    "                  metrics=[CustomMetric])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:47:47.632613Z",
     "start_time": "2024-05-16T07:47:47.628112Z"
    }
   },
   "id": "e12a305f958d6056"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\nValue passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=tf.Tensor(shape=(None, 1), dtype=float32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(None, 1), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mregressor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "Cell \u001B[0;32mIn[16], line 10\u001B[0m, in \u001B[0;36mBayesianBertRegressor.call\u001B[0;34m(self, input_ids)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids):\n\u001B[0;32m---> 10\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][:, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     11\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(output, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m     mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_layer(output)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:437\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[1;32m    436\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[0;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43munpacked_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:1234\u001B[0m, in \u001B[0;36mTFBertModel.call\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;129m@unpack_inputs\u001B[39m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;129m@add_start_docstrings_to_model_forward\u001B[39m(BERT_INPUTS_DOCSTRING\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size, sequence_length\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;129m@add_code_sample_docstrings\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1212\u001B[0m     training: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1213\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001B[39;00m\n\u001B[1;32m   1233\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1234\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1249\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1250\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:437\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[1;32m    436\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[0;32m--> 437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43munpacked_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:912\u001B[0m, in \u001B[0;36mTFBertMainLayer.call\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[1;32m    909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token_type_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    910\u001B[0m     token_type_ids \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mfill(dims\u001B[38;5;241m=\u001B[39minput_shape, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 912\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    913\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    914\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    921\u001B[0m \u001B[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001B[39;00m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001B[39;00m\n\u001B[1;32m    923\u001B[0m \u001B[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001B[39;00m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001B[39;00m\n\u001B[1;32m    925\u001B[0m \u001B[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001B[39;00m\n\u001B[1;32m    926\u001B[0m attention_mask_shape \u001B[38;5;241m=\u001B[39m shape_list(attention_mask)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:207\u001B[0m, in \u001B[0;36mTFBertEmbeddings.call\u001B[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     check_embeddings_within_bounds(input_ids, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mvocab_size)\n\u001B[0;32m--> 207\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m shape_list(inputs_embeds)[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token_type_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mTypeError\u001B[0m: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).\n\nValue passed to parameter 'indices' has DataType float32 not in list of allowed values: int32, int64\n\nCall arguments received by layer 'embeddings' (type TFBertEmbeddings):\n  • input_ids=tf.Tensor(shape=(None, 1), dtype=float32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(None, 1), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
     ]
    }
   ],
   "source": [
    "history = regressor.fit(X_train, (y_train, y_train),\n",
    "                        validation_data=(X_val, (y_val, y_val)),\n",
    "                        epochs=num_epochs,\n",
    "                        batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T07:47:49.199281Z",
     "start_time": "2024-05-16T07:47:48.907391Z"
    }
   },
   "id": "194f769f16380a3d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "n_trees_values = [25, 50, 80]\n",
    "max_depth_values = [3, 5, 7]\n",
    "n_estimators_values = [30, 50, 80]\n",
    "subportion_values = [0.5, 0.7, 0.9]\n",
    "n_runs_values = [20, 50, 80]\n",
    "\n",
    "# Define divisor values based on n_estimators\n",
    "divisors_by_n_estimators = {\n",
    "    30: [3, 5, 7],\n",
    "    50: [3, 5, 7],\n",
    "    80: [3, 5, 7],\n",
    "    120: [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Generate parameter combinations\n",
    "parameter_combinations = []\n",
    "for n_trees, max_depth, n_estimators, subportion, n_runs in itertools.product(\n",
    "    n_trees_values, max_depth_values, n_estimators_values, subportion_values, n_runs_values\n",
    "):\n",
    "    if n_estimators in divisors_by_n_estimators:\n",
    "        divisors = divisors_by_n_estimators[n_estimators]\n",
    "        for divisor in divisors:\n",
    "            n_samples = n_estimators // divisor\n",
    "            parameter_combinations.append((n_trees, max_depth, n_estimators, subportion, n_samples, n_runs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T14:16:44.025293Z",
     "start_time": "2024-04-16T14:16:44.021258Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.15\n"
     ]
    }
   ],
   "source": [
    "print(len(parameter_combinations)/60)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T14:16:59.639042Z",
     "start_time": "2024-04-16T14:16:59.624769Z"
    }
   },
   "id": "7244022a627256b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "914d8e86592ab95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
