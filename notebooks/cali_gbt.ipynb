{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T08:43:22.611853Z",
     "start_time": "2024-05-02T08:43:03.039478Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cali_housing_path = '../data/California_Houses.csv'\n",
    "RANDOM_SEED = 492"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T08:49:29.540951Z",
     "start_time": "2024-05-02T08:49:29.534524Z"
    }
   },
   "id": "7d034f24c61b4ead"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "cali_df = pd.read_csv(cali_housing_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T08:45:47.269563Z",
     "start_time": "2024-05-02T08:45:47.244667Z"
    }
   },
   "id": "e67f3515498765c5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Median_House_Value  Median_Income  Median_Age  Tot_Rooms  Tot_Bedrooms  \\\n",
      "0            452600.0         8.3252          41        880           129   \n",
      "1            358500.0         8.3014          21       7099          1106   \n",
      "2            352100.0         7.2574          52       1467           190   \n",
      "3            341300.0         5.6431          52       1274           235   \n",
      "4            342200.0         3.8462          52       1627           280   \n",
      "\n",
      "   Population  Households  Latitude  Longitude  Distance_to_coast  \\\n",
      "0         322         126     37.88    -122.23        9263.040773   \n",
      "1        2401        1138     37.86    -122.22       10225.733072   \n",
      "2         496         177     37.85    -122.24        8259.085109   \n",
      "3         558         219     37.85    -122.25        7768.086571   \n",
      "4         565         259     37.85    -122.25        7768.086571   \n",
      "\n",
      "   Distance_to_LA  Distance_to_SanDiego  Distance_to_SanJose  \\\n",
      "0   556529.158342         735501.806984         67432.517001   \n",
      "1   554279.850069         733236.884360         65049.908574   \n",
      "2   554610.717069         733525.682937         64867.289833   \n",
      "3   555194.266086         734095.290744         65287.138412   \n",
      "4   555194.266086         734095.290744         65287.138412   \n",
      "\n",
      "   Distance_to_SanFrancisco  \n",
      "0              21250.213767  \n",
      "1              20880.600400  \n",
      "2              18811.487450  \n",
      "3              18031.047568  \n",
      "4              18031.047568  \n"
     ]
    }
   ],
   "source": [
    "print(cali_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T08:46:00.164282Z",
     "start_time": "2024-05-02T08:46:00.158307Z"
    }
   },
   "id": "dd3a9aebe1b6f202"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "y_series = cali_df['Median_House_Value']\n",
    "y = pd.DataFrame(y_series, columns=['Median_House_Value'])\n",
    "features = [col for col in cali_df.columns if col != 'Median_House_Value']\n",
    "X = cali_df[features]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:16:42.106810Z",
     "start_time": "2024-05-02T09:16:42.096117Z"
    }
   },
   "id": "6ac9273c8f8fd518"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:16:52.980288Z",
     "start_time": "2024-05-02T09:16:52.968028Z"
    }
   },
   "id": "aecb071343afa4b4"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:16:54.242156Z",
     "start_time": "2024-05-02T09:16:54.234152Z"
    }
   },
   "id": "c4eb199e83a8880"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=16512, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:16:55.288373Z",
     "start_time": "2024-05-02T09:16:55.285064Z"
    }
   },
   "id": "5668d13c68492363"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class GradientBoostedTreesEnsembleRegressor(tf.keras.Model):\n",
    "    def __init__(self, n_trees=100, max_depth=3, n_estimators=100, sample_method='RANDOM'): \n",
    "        super(GradientBoostedTreesEnsembleRegressor, self).__init__()\n",
    "        self.n_trees = n_trees\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.sample_method = sample_method\n",
    "        self.estimators = []\n",
    "\n",
    "    def build_estimator(self):\n",
    "        model = tfdf.keras.GradientBoostedTreesModel(\n",
    "            task=tfdf.keras.Task.REGRESSION,\n",
    "            num_trees=self.n_trees,\n",
    "            max_depth=np.random.randint(self.max_depth//2, self.max_depth+1),\n",
    "            validation_ratio=0.1,\n",
    "            sampling_method=self.sample_method,\n",
    "            random_seed=self.seed # try to set this first. Maybe this is enough\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # if not, the number of trees/depth different for each estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_data = X.shape[0]\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X.values, y.values))\n",
    "        \n",
    "        for seed in np.random.randint(0, 1000000, size=self.n_estimators):\n",
    "            self.seed = int(seed)\n",
    "            train_estimator_indices = np.random.choice(num_data, size=num_data, replace=False)\n",
    "            \n",
    "            # Create a subset of the dataset using sampled indices\n",
    "            subset_dataset = dataset.enumerate().filter(lambda i, data: tf.reduce_any(i == train_estimator_indices))\n",
    "\n",
    "            # building each estimator\n",
    "            estimator = self.build_estimator()\n",
    "            estimator.fit(subset_dataset)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X): # use all estimators\n",
    "        predictions = []\n",
    "        # sampled_estimators = np.random.choice(self.estimators, size=n_samples, replace=False)\n",
    "        for estimator in self.estimators:\n",
    "            estimator_predictions = []\n",
    "            estimator_predictions.append(estimator.predict(X))\n",
    "            estimator_predictions = tf.stack(estimator_predictions, axis=-1)\n",
    "            predictions.append(tf.reduce_mean(estimator_predictions, axis=-1))\n",
    "        predictions = tf.stack(predictions, axis=0)\n",
    "        \n",
    "        mean_prediction = tf.reduce_mean(predictions, axis=0)\n",
    "        variance_prediction = tf.math.reduce_std(predictions, axis=0)\n",
    "        return mean_prediction, variance_prediction\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:25:03.505162Z",
     "start_time": "2024-05-02T09:25:03.501471Z"
    }
   },
   "id": "919970c901a09027"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "gbt_model = GradientBoostedTreesEnsembleRegressor(\n",
    "    n_trees=50, \n",
    "    max_depth=5, \n",
    "    n_estimators=80, \n",
    "    # subportion=0.3, \n",
    "    sample_method='RANDOM'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:25:04.432565Z",
     "start_time": "2024-05-02T09:25:04.429520Z"
    }
   },
   "id": "29337526f5a1fc2b"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /var/folders/3r/pzt2_p_x2xdfl2057nxkp3nm0000gn/T/tmpbpz9rt4r as temporary training directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 24-05-02 11:25:05.0747 CEST gradient_boosted_trees.cc:1840] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "[WARNING 24-05-02 11:25:05.0748 CEST gradient_boosted_trees.cc:1851] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "[WARNING 24-05-02 11:25:05.0748 CEST gradient_boosted_trees.cc:1865] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The dataset does not contain a 'batch' operation. TF-DF models should be trained with batch operations. Add a batch operations to solve this issue. Alternatively, you can disabled this check with the constructor argument `check_dataset=False`. If this message is a false positive, please let us know so we can improve this dataset check logic.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgbt_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[64], line 37\u001B[0m, in \u001B[0;36mGradientBoostedTreesEnsembleRegressor.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# building each estimator\u001B[39;00m\n\u001B[1;32m     36\u001B[0m estimator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuild_estimator()\n\u001B[0;32m---> 37\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators\u001B[38;5;241m.\u001B[39mappend(estimator)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core.py:1301\u001B[0m, in \u001B[0;36mCoreModel.fit\u001B[0;34m(self, x, y, callbacks, verbose, validation_steps, validation_data, sample_weight, steps_per_epoch, class_weight, **kwargs)\u001B[0m\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;66;03m# Check the dataset.\u001B[39;00m\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_dataset \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset):\n\u001B[0;32m-> 1301\u001B[0m   \u001B[43m_check_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;66;03m# Call \"compile\" if the user forgot to do so.\u001B[39;00m\n\u001B[1;32m   1304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_compiled:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core.py:2525\u001B[0m, in \u001B[0;36m_check_dataset\u001B[0;34m(ds)\u001B[0m\n\u001B[1;32m   2523\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(ds)\n\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ds, load_op\u001B[38;5;241m.\u001B[39m_LoadDataset) \u001B[38;5;129;01mand\u001B[39;00m batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m-> 2525\u001B[0m   \u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2526\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe dataset does not contain a \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m operation. TF-DF models \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m   2527\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshould be trained with batch operations. Add a batch operations \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m   2528\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mto solve this issue.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m   2529\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2531\u001B[0m num_examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2532\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/program-i-Fez0zm-py3.10/lib/python3.10/site-packages/tensorflow_decision_forests/keras/core.py:2504\u001B[0m, in \u001B[0;36m_check_dataset.<locals>.error\u001B[0;34m(message)\u001B[0m\n\u001B[1;32m   2502\u001B[0m   tf_logging\u001B[38;5;241m.\u001B[39mwarning(message)\n\u001B[1;32m   2503\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2504\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[0;31mValueError\u001B[0m: The dataset does not contain a 'batch' operation. TF-DF models should be trained with batch operations. Add a batch operations to solve this issue. Alternatively, you can disabled this check with the constructor argument `check_dataset=False`. If this message is a false positive, please let us know so we can improve this dataset check logic."
     ]
    }
   ],
   "source": [
    "gbt_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T09:25:05.130100Z",
     "start_time": "2024-05-02T09:25:05.052512Z"
    }
   },
   "id": "863d540db7ca3707"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7f14d9cb64beec5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
