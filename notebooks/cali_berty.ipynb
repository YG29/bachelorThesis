{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-16T15:15:10.329036Z",
     "start_time": "2024-06-16T15:15:07.106813Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load and preprocess data: cali housing\n",
    "\n",
    "cali_housing_path = '../data/California_Houses.csv'\n",
    "RANDOM_SEED = 492\n",
    "cali_df = pd.read_csv(cali_housing_path)\n",
    "y_series = cali_df['Median_House_Value']\n",
    "y = pd.DataFrame(y_series, columns=['Median_House_Value'])\n",
    "features = [col for col in cali_df.columns if col != 'Median_House_Value']\n",
    "X = cali_df[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# preprocessing\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T15:15:15.618172Z",
     "start_time": "2024-06-16T15:15:15.587592Z"
    }
   },
   "id": "46a7beef9170b5d9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# make the data into strings\n",
    "X_train_texts = X_train_scaled.astype(str).apply(' '.join, axis=1).tolist()\n",
    "X_test_texts = X_test_scaled.astype(str).apply(' '.join, axis=1).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T15:15:16.814171Z",
     "start_time": "2024-06-16T15:15:16.807342Z"
    }
   },
   "id": "321ad9fe2c115842"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T15:15:19.104216Z",
     "start_time": "2024-06-16T15:15:17.466701Z"
    }
   },
   "id": "bb0d6a343b164e6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BERTRegression(Model):\n",
    "    def __init__(self, max_length, dense_size, dropout_rate=0.1, num_mc_samples=10, num_features=13):\n",
    "        super(BERTRegression, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model.trainable = True\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.dense_layer = Dense(dense_size, activation='relu')\n",
    "        self.mc_dropout = Dropout(dropout_rate)\n",
    "        self.output_layer = Dense(1, activation='linear')\n",
    "        self.num_mc_samples = num_mc_samples\n",
    "        self.explainer = LimeTextExplainer()\n",
    "        self.num_features = num_features\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        bert_output = self.bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[1]\n",
    "        dropout_output = self.dropout(bert_output, training=training)\n",
    "        hidden_output = self.dense_layer(dropout_output)\n",
    "        mc_dropout_output = self.mc_dropout(hidden_output, training=training)\n",
    "        output = self.output_layer(mc_dropout_output)\n",
    "\n",
    "        if training:\n",
    "            return output\n",
    "        else:\n",
    "            output_samples = tf.stack([self(inputs, training=False) for _ in range(self.num_mc_samples)])\n",
    "            output_mean = tf.reduce_mean(output_samples, axis=0)\n",
    "            output_sd = tf.math.reduce_std(output_samples, axis=0)\n",
    "            return output_mean, output_sd\n",
    "\n",
    "    def predict_with_uncertainty(self, inputs):\n",
    "        output_mean, output_sd = self(inputs, training=False)\n",
    "        return output_mean, output_sd\n",
    "\n",
    "    def explain_lime(self, text_instance):\n",
    "        explanations = []\n",
    "\n",
    "        def predict_function(texts):\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "                texts,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            outputs = self.predict_with_uncertainty([\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask'],\n",
    "                inputs['token_type_ids']\n",
    "            ])\n",
    "            return outputs[0].numpy()\n",
    "\n",
    "        exp = self.explainer.explain_instance(\n",
    "            text_instance,\n",
    "            predict_function,\n",
    "            num_features=self.num_features\n",
    "        )\n",
    "        explanations.append(exp)\n",
    "        return explanations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T15:15:21.390512Z",
     "start_time": "2024-06-16T15:15:21.386354Z"
    }
   },
   "id": "1c28c76d65495ee9"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# set values\n",
    "max_length = 230\n",
    "dense_size = 55\n",
    "dropout_rate = 0.1\n",
    "num_mc_samples = 50\n",
    "num_features = 13"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:02:03.460652Z",
     "start_time": "2024-06-16T18:02:03.449343Z"
    }
   },
   "id": "e4ae443595a81c3d"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "berty_pilot = BERTRegression(max_length, dense_size, dropout_rate, num_mc_samples, num_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:02:11.870729Z",
     "start_time": "2024-06-16T18:02:10.109373Z"
    }
   },
   "id": "da724b82b1b40eec"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Tokenizer for encoding inputs\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:02:38.331615Z",
     "start_time": "2024-06-16T18:02:38.191995Z"
    }
   },
   "id": "271471f45e51a761"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tokenized length: 229\n"
     ]
    }
   ],
   "source": [
    "# Function to concatenate column names and values\n",
    "def concatenate_columns_with_names(df):\n",
    "    concatenated = df.apply(lambda row: ' '.join([f\"{col} {val}\" for col, val in row.items()]), axis=1)\n",
    "    return concatenated.tolist()\n",
    "# Concatenate column names and values for each instance\n",
    "X_train_texts_with_names = concatenate_columns_with_names(X_train_scaled)\n",
    "\n",
    "# Calculate the maximum tokenized length\n",
    "max_tokenized_length = 0\n",
    "for text in X_train_texts_with_names:\n",
    "    encoded_input = tokenizer.encode(text, add_special_tokens=True)  # add_special_tokens adds [CLS] and [SEP]\n",
    "    tokenized_length = len(encoded_input)\n",
    "    if tokenized_length > max_tokenized_length:\n",
    "        max_tokenized_length = tokenized_length\n",
    "\n",
    "print(f\"Maximum tokenized length: {max_tokenized_length}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:01:44.786466Z",
     "start_time": "2024-06-16T18:01:27.444455Z"
    }
   },
   "id": "ea04e9d100f64f1d"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "\n",
    "# Encode the concatenated inputs\n",
    "encoded_inputs_train_cn = tokenizer.batch_encode_plus(\n",
    "    X_train_texts_with_names,\n",
    "    max_length=max_length,  \n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:03:26.463183Z",
     "start_time": "2024-06-16T18:03:07.116850Z"
    }
   },
   "id": "c1c34e012db4f009"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Median_Income -0.22023367473901587 Median_Age 0.18302885044421452 Tot_Rooms -0.505329788280167 Tot_Bedrooms -0.5832710453838817 Population -0.625332763570887 Households -0.5546840263301412 Latitude 1.3115180113240754 Longitude -1.6397854101401088 Distance_to_coast -0.3915220068123694 Distance_to_LA 1.4909602397221111 Distance_to_SanDiego 1.4493012599111588 Distance_to_SanJose -0.9188291839326336 Distance_to_SanFrancisco -1.2106935875117162']\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "print(X_train_texts_with_names[:1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:03:29.684851Z",
     "start_time": "2024-06-16T18:03:29.681046Z"
    }
   },
   "id": "2b13ec6db549bb30"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 230), dtype=int32, numpy=\n",
      "array([[  101,  3991,  1035,  3318,  1011,  1014,  1012, 10545, 21926,\n",
      "        21619,  2581, 22610, 23499, 24096, 27814,  2581,  3991,  1035,\n",
      "         2287,  1014,  1012,  9500, 22407, 27531,  2692, 22932, 20958,\n",
      "        16932, 25746,  2000,  2102,  1035,  4734,  1011,  1014,  1012,\n",
      "        28952, 16703,  2683,  2581,  2620,  2620, 22407, 24096,  2575,\n",
      "         2581,  2000,  2102,  1035, 18390,  1011,  1014,  1012,  5388,\n",
      "        16703,  2581, 10790, 19961, 22025, 22025,  2620, 16576,  2313,\n",
      "         1011,  1014,  1012, 22810, 22394, 22907,  2575, 19481, 19841,\n",
      "         2620,  2620,  2581,  3911,  1011,  1014,  1012,  4583, 21472,\n",
      "         2620, 12740, 23833, 22394, 24096, 23632,  2475, 15250,  1015,\n",
      "         1012, 23532, 22203, 17914, 14526, 16703, 12740, 23352,  2549,\n",
      "        20413,  1011,  1015,  1012,  6191,  2683,  2581, 27531, 23632,\n",
      "        24096, 12740, 10790,  2620,  2620,  3292,  1035,  2000,  1035,\n",
      "         3023,  1011,  1014,  1012,  4464, 16068, 19317,  8889,  2575,\n",
      "         2620, 12521, 21619,  2683,  2549,  3292,  1035,  2000,  1035,\n",
      "         2474,  1015,  1012, 22288,  2683, 16086, 21926,  2683,  2581,\n",
      "        19317, 14526, 14526,  3292,  1035,  2000,  1035,  5472, 28872,\n",
      "         2080,  1015,  1012,  4008,  2683, 14142, 12521, 28154,  2683,\n",
      "        14526, 16068,  2620,  2620,  3292,  1035,  2000,  1035,  2624,\n",
      "        19929,  2063,  1011,  1014,  1012,  6205,  2620,  2620, 24594,\n",
      "        15136, 23499, 16703,  2575, 22394,  2575,  3292,  1035,  2000,\n",
      "         1035,  2624, 27843, 12273,  2483,  3597,  1011,  1015,  1012,\n",
      "        12875,  2575,  2683, 19481,  2620, 23352, 14526,  2581, 16048,\n",
      "         2475,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [  101,  3991,  1035,  3318,  1015,  1012, 14677,  2581, 16147,\n",
      "        28756, 22610,  2692, 21926, 24096,  3991,  1035,  2287,  1011,\n",
      "         1014,  1012,  3429, 22394, 22203,  2683, 12740,  2683,  2683,\n",
      "        24594,  2692, 21486,  2000,  2102,  1035,  4734,  1011,  1014,\n",
      "         1012, 28161,  2581,  2683,  2575, 20842,  2620, 23777, 21057,\n",
      "        24434,  2683,  2549,  2000,  2102,  1035, 18390,  1011,  1014,\n",
      "         1012, 26499, 16048, 16086, 26976, 17465, 17134, 24096,  2549,\n",
      "         2313,  1011,  1014,  1012,  6255,  2683,  2581, 18827,  2683,\n",
      "         2581,  2620, 22022,  2581,  2581,  2683, 22275,  3911,  1011,\n",
      "         1014,  1012,  4413,  2620, 22610, 20842, 21472, 25746, 23833,\n",
      "         2620, 14526,  2549, 15250,  1015,  1012,  5709, 20958, 19317,\n",
      "        12740, 22394, 22025,  2620,  2683,  2475, 20413,  1011,  1015,\n",
      "         1012,  3429, 12376, 28311, 22203, 27009,  2692,  2683, 20842,\n",
      "         2475,  3292,  1035,  2000,  1035,  3023,  1011,  1014,  1012,\n",
      "         6352, 16147,  2620,  2581, 19481, 26976,  2683,  2683, 28311,\n",
      "         2620,  3292,  1035,  2000,  1035,  2474,  1015,  1012, 10545,\n",
      "         2683, 28756,  2581,  2683, 22932, 16703,  2620,  2683, 28756,\n",
      "         3292,  1035,  2000,  1035,  5472, 28872,  2080,  1015,  1012,\n",
      "        20294,  2620,  2620,  2620, 22610, 19317, 21486,  2581,  2575,\n",
      "        24434,  3292,  1035,  2000,  1035,  2624, 19929,  2063,  1011,\n",
      "         1015,  1012, 22115, 11387, 21926, 21486,  2575, 16068, 10790,\n",
      "         2581,  2509,  3292,  1035,  2000,  1035,  2624, 27843, 12273,\n",
      "         2483,  3597,  1011,  1015,  1012,  4466,  2575,  2620, 27531,\n",
      "         2683,  2683,  2575, 22907, 21619,  2683, 17134,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 230), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 230), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "print(encoded_inputs_train_cn[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:03:32.806890Z",
     "start_time": "2024-06-16T18:03:32.797942Z"
    }
   },
   "id": "bcc114c5c1623c06"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "berty_pilot.compile(optimizer='adam', loss='mse')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T18:03:51.121781Z",
     "start_time": "2024-06-16T18:03:51.112655Z"
    }
   },
   "id": "1dee2bbf549a5761"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m413/413\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3s/step - loss: 56196300800.0000"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = berty_pilot.fit(\n",
    "    [encoded_inputs_train_cn['input_ids'], encoded_inputs_train_cn['attention_mask'], encoded_inputs_train_cn['token_type_ids']],\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-16T18:05:46.743920Z"
    }
   },
   "id": "e33632e70a56e377"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tot_history=[]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T17:48:42.263480Z",
     "start_time": "2024-06-16T17:48:42.259784Z"
    }
   },
   "id": "93b515aaba2af12c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tot_history\u001B[38;5;241m=\u001B[39mtot_history\u001B[38;5;241m.\u001B[39mappend(\u001B[43mhistory\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "tot_history=tot_history.append(history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T17:48:44.379130Z",
     "start_time": "2024-06-16T17:48:44.363605Z"
    }
   },
   "id": "1f4fb93e09349927"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Plot training & validation loss values\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mhistory\u001B[49m\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel loss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-16T17:48:52.282155Z",
     "start_time": "2024-06-16T17:48:52.272256Z"
    }
   },
   "id": "84381e1f549be39f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "berty_pilot.save('models/bert_cali_model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b41fda06ed50f0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Custom object dictionary\n",
    "custom_model = {'BERTRegression': BERTRegression}\n",
    "\n",
    "# Load the model with custom objects\n",
    "berty_pilot = load_model('path_to_saved_model', custom_objects=custom_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf72c68ee912b8e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example test instance\n",
    "instance_index = 0\n",
    "X_test_instance = X_test_scaled.iloc[instance_index]\n",
    "true_value = y_test.values[instance_index]\n",
    "X_test_instance_with_names = concatenate_columns_with_names(X_test_instance)\n",
    "\n",
    "# Tokenize and encode the instance\n",
    "test_inputs = tokenizer.encode_plus(\n",
    "    X_test_instance_with_names,\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d637f9c11419881d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get explanation\n",
    "explanations = bert_model.explain_lime(test_inputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d93c6c563fa57daf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print explanations\n",
    "for exp in explanations:\n",
    "    print(exp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682b7a2387a33847"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
